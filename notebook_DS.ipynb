{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PySPARQL==0.0.5\n",
      "  Downloading PySPARQL-0.0.5-py3-none-any.whl (6.0 kB)\n",
      "Collecting graphframes\n",
      "  Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from graphframes->PySPARQL==0.0.5) (1.19.4)\n",
      "Collecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyspark\n",
      "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 204.2 MB 61 kB/s s eta 0:00:01     |███████████████████████▊        | 151.6 MB 10.7 MB/s eta 0:00:05\n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rdflib\n",
      "  Downloading rdflib-5.0.0-py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from rdflib->PySPARQL==0.0.5) (1.15.0)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.8/site-packages (from rdflib->PySPARQL==0.0.5) (2.4.7)\n",
      "Collecting isodate\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from rdflib->PySPARQL==0.0.5) (1.15.0)\n",
      "Collecting rdflib-jsonld\n",
      "  Downloading rdflib-jsonld-0.5.0.tar.gz (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 5.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sparqlwrapper\n",
      "  Downloading SPARQLWrapper-1.8.5-py3-none-any.whl (26 kB)\n",
      "Building wheels for collected packages: pyspark, rdflib-jsonld\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=2867ba7baef7085ad32aace76cce39dee7d54069345b34322818d79369dc11e8\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/ea/21/84/970b03913d0d6a96ef51c34c878add0de9e4ecbb7c764ea21f\n",
      "  Building wheel for rdflib-jsonld (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rdflib-jsonld: filename=rdflib_jsonld-0.5.0-py2.py3-none-any.whl size=15347 sha256=36a98654f418354bbfa5ee5ebc18efe5ff314265c6391ad18f6fb1462d90c342\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/3a/97/90/e133cbb98e344c2ca55120f8d704f6ff57bdfd8e30f1dc5451\n",
      "Successfully built pyspark rdflib-jsonld\n",
      "Installing collected packages: isodate, rdflib, py4j, nose, sparqlwrapper, rdflib-jsonld, pyspark, graphframes, PySPARQL\n",
      "Successfully installed PySPARQL-0.0.5 graphframes-0.6 isodate-0.6.0 nose-1.3.7 py4j-0.10.9 pyspark-3.0.1 rdflib-5.0.0 rdflib-jsonld-0.5.0 sparqlwrapper-1.8.5\n"
     ]
    }
   ],
   "source": [
    "!pip install PySPARQL==0.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PySPARQL.Wrapper import PySPARQLWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "          .builder\n",
    "          .appName(\"interfacing spark sql to hive metastore without configuration file\")\n",
    "          .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \n",
    "          .enableHiveSupport() \n",
    "          .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARQL query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizzaID,outcome\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ789,ANOMALY\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ333,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ222,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ005,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ456,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ001,ANOMALY\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ789,ANOMALY\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ444,ANOMALY\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ333,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ999,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ111,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ003,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ004,ANOMALY\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ002,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ888,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ666,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ777,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ123,ANOMALY\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ555,WELL_COOKED\r\n",
      "http://www.co-ode.org/ontologies/pizza/pizza.owl#PZ222,ANOMALY\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparql_endpoint = \"http://jena-fuseki:3030/pizzads\"\n",
    "\n",
    "query = \"\"\"\n",
    "    PREFIX : <http://www.co-ode.org/ontologies/pizza/pizza.owl#>\n",
    "\n",
    "    SELECT ?pizzaID ?outcome\n",
    "    WHERE {\n",
    "\n",
    "      ?pizzaType :suggestedTempLow ?tempLow; :suggestedDurationLow ?durLow;\n",
    "                 :suggestedTempUp ?tempUp; :suggestedDurationUp ?durUp .\n",
    "\n",
    "      SERVICE <http://ontop:8080/sparql> {\n",
    "        ?pizzaID a ?pizzaType.\n",
    "        ?pizzaID :temperature ?avgTemp; :start_cooking ?start; :end_cooking ?end.\n",
    "      }\n",
    "\n",
    "      BIND ((?end-?start) AS ?cookDuration)\n",
    "\n",
    "      BIND( IF ((?avgTemp >= ?tempLow && ?avgTemp <= ?tempUp) &&\n",
    "          (?cookDuration >= ?durLow && ?cookDuration <= ?durUp)\n",
    "          ,\"WELL_COOKED\",\"ANOMALY\") AS ?outcome)\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "wrapper = PySPARQLWrapper(spark, sparql_endpoint)\n",
    "result = wrapper.query(query)\n",
    "resultDF = result.dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|             pizzaID|    outcome|\n",
      "+--------------------+-----------+\n",
      "|http://www.co-ode...|    ANOMALY|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|    ANOMALY|\n",
      "|http://www.co-ode...|    ANOMALY|\n",
      "|http://www.co-ode...|    ANOMALY|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|    ANOMALY|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|    ANOMALY|\n",
      "|http://www.co-ode...|WELL_COOKED|\n",
      "|http://www.co-ode...|    ANOMALY|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF.show()  # Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the result by removing the prefixes\n",
    "\n",
    "df2 = resultDF.withColumn(\"pizzaID\", regexp_replace('pizzaID','http://www.co-ode.org/ontologies/pizza/pizza.owl#',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|pizzaID|    outcome|\n",
      "+-------+-----------+\n",
      "|  PZ789|    ANOMALY|\n",
      "|  PZ333|WELL_COOKED|\n",
      "|  PZ222|WELL_COOKED|\n",
      "|  PZ005|WELL_COOKED|\n",
      "|  PZ456|WELL_COOKED|\n",
      "|  PZ001|    ANOMALY|\n",
      "|  PZ789|    ANOMALY|\n",
      "|  PZ444|    ANOMALY|\n",
      "|  PZ333|WELL_COOKED|\n",
      "|  PZ999|WELL_COOKED|\n",
      "|  PZ111|WELL_COOKED|\n",
      "|  PZ003|WELL_COOKED|\n",
      "|  PZ004|    ANOMALY|\n",
      "|  PZ002|WELL_COOKED|\n",
      "|  PZ888|WELL_COOKED|\n",
      "|  PZ666|WELL_COOKED|\n",
      "|  PZ777|WELL_COOKED|\n",
      "|  PZ123|    ANOMALY|\n",
      "|  PZ555|WELL_COOKED|\n",
      "|  PZ222|    ANOMALY|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()   # Plot the cleaned results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persists the Spark Dataframe into a Spark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.mode(\"overwrite\").saveAsTable('pizzadb.analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
